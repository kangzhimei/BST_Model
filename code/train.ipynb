{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.base import BaseEstimator\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import os\n",
    "import pickle\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import math\n",
    "import openpyxl\n",
    "import matplotlib.font_manager as fm \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tre:\n",
    "    def __init__(self):\n",
    "        self.root = {}\n",
    "\n",
    "    def insert(self, sequence):\n",
    "        node = self.root\n",
    "        for item in sequence:\n",
    "            if item not in node:\n",
    "                node[item] = {}\n",
    "            node = node[item]\n",
    "        node['end'] = True\n",
    "\n",
    "    def search(self, sequence):\n",
    "        node = self.root\n",
    "        for item in sequence:\n",
    "            if item not in node:\n",
    "                return False\n",
    "            node = node[item]\n",
    "        return 'end' in node\n",
    "\n",
    "    def extract_features(self, sequence):\n",
    "        features = []\n",
    "        for i in range(len(sequence)):\n",
    "            for j in range(i+1, len(sequence)+1):\n",
    "                subsequence = sequence[i:j]\n",
    "                if self.search(subsequence):\n",
    "                    features.append(subsequence)\n",
    "                else:\n",
    "                    break\n",
    "        return features\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.hidden_dim)\n",
    "        attention_weights = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attended_values = torch.matmul(attention_weights, value)\n",
    "\n",
    "        return attended_values\n",
    "\n",
    "class WellLoggingDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, num_layers, output_dim):\n",
    "        super(LSTMRegressor, self).__init__()\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dim1, 1, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim1 * 2, hidden_dim2, 1, batch_first=True, bidirectional=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim2 * 2, hidden_dim3, 1, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim3 * 2, output_dim)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = src.unsqueeze(1)\n",
    "\n",
    "        lstm_output1, _ = self.lstm1(src)\n",
    "        lstm_output2, _ = self.lstm2(lstm_output1)\n",
    "        lstm_output3, _ = self.lstm3(lstm_output2)\n",
    "\n",
    "        output = self.fc(lstm_output3[:, -1, :])\n",
    "\n",
    "        return output\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, learning_rate, patience):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_params = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        batch_losses = []\n",
    "        for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "        epoch_loss = sum(batch_losses) / len(batch_losses)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_params = model.state_dict()\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_params)\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "def test_model(model, test_loader, learning_rate, num_iterations):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "    for iteration in range(num_iterations):\n",
    "        for batch_idx, (features, labels) in enumerate(test_loader):\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    test_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in test_loader:\n",
    "            features = features.to(device)\n",
    "            outputs = model(features)\n",
    "            test_predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "    return np.array(test_predictions)\n",
    "\n",
    "def preprocess_data(features, target, test_size=0.3, random_state=42):\n",
    "    feature_scaler = RobustScaler()\n",
    "    target_scaler = RobustScaler()\n",
    "\n",
    "    features_scaled = feature_scaler.fit_transform(features)\n",
    "    target_scaled = target_scaler.fit_transform(target.reshape(-1, 1))\n",
    "\n",
    "    features_train, features_test, target_train, target_test = train_test_split(\n",
    "        features_scaled, target_scaled, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return features_train, features_test, target_train, target_test, feature_scaler, target_scaler\n",
    "\n",
    "def extract_tre_features(data, max_length):\n",
    "    tre = Tre()\n",
    "    for sequence in data:\n",
    "        tre.insert(sequence)\n",
    "\n",
    "    tre_features = []\n",
    "    for sequence in data:\n",
    "        features = tre.extract_features(sequence)\n",
    "        feature_vector = {}\n",
    "        for subsequence in features:\n",
    "            if len(subsequence) <= max_length:\n",
    "                feature_vector[tuple(subsequence)] = 1\n",
    "        tre_features.append(feature_vector)\n",
    "\n",
    "    num_samples = len(tre_features)\n",
    "    num_features = max(len(feature_dict) for feature_dict in tre_features)\n",
    "    ttt_features_array = np.zeros((num_samples, num_features), dtype=int)\n",
    "    for i, feature_dict in enumerate(tre_features):\n",
    "        for j, (subsequence, value) in enumerate(feature_dict.items()):\n",
    "            ttt_features_array[i, j] = value\n",
    "\n",
    "    return ttt_features_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(5, 6):\n",
    "    misses = ['DEPTH', 'GR', 'RS', 'RD', 'DEN', 'DTC']\n",
    "    feature_names = misses[:step] + misses[step+1:]\n",
    "    targ = misses[step]\n",
    "    print('calculate_missing:', targ)\n",
    "\n",
    "    file_path = r'/Recon_Model-main/data/end_data.csv'\n",
    "    training_data = pd.read_csv(file_path)\n",
    "\n",
    "    features = training_data[feature_names].values\n",
    "    target = training_data[targ].values.reshape(-1, 1)\n",
    "    layer_feature = training_data['Layer'] \n",
    "\n",
    "    onehot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    feature_scaler = RobustScaler()\n",
    "    target_scaler = RobustScaler()\n",
    "    features = np.nan_to_num(features)\n",
    "    target = np.nan_to_num(target)\n",
    "\n",
    "    # Hot coding layer column\n",
    "    layer_onehot = onehot_encoder.fit_transform(layer_feature.values.reshape(-1, 1)).toarray()\n",
    "    features_encoded = np.concatenate((features, layer_onehot), axis=1)\n",
    "\n",
    "    # normalization\n",
    "    features_scaled = feature_scaler.fit_transform(features_encoded)\n",
    "    target_scaled = target_scaler.fit_transform(target)\n",
    "\n",
    "    # Split data\n",
    "    features_train, features_test, target_train, target_test = train_test_split(features_scaled, target_scaled, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Extract features\n",
    "    max_length = 5\n",
    "    tre_features_train = extract_tre_features(features_train, max_length)\n",
    "    tre_features_test = extract_tre_features(features_test, max_length)\n",
    "\n",
    "    # Combine features with original features\n",
    "    features_train_combined = np.concatenate((features_train, tre_features_train), axis=1)\n",
    "    features_test_combined = np.concatenate((features_test, tre_features_test), axis=1)\n",
    "\n",
    "    train_dataset = WellLoggingDataset(features_train_combined, target_train)\n",
    "    val_dataset = WellLoggingDataset(features_test_combined, target_test)\n",
    "    test_dataset = WellLoggingDataset(features_test_combined, target_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "    input_dim = features_train_combined.shape[1]\n",
    "    hidden_dim1 = 256\n",
    "    hidden_dim2 = 512\n",
    "    hidden_dim3 = 256\n",
    "    num_layers = 5\n",
    "    output_dim = 1\n",
    "    epochs = 90\n",
    "    learning_rate = 0.001\n",
    "    patience = 10\n",
    "\n",
    "    model = LSTMRegressor(input_dim, hidden_dim1, hidden_dim2, hidden_dim3, num_layers, output_dim)\n",
    "    model, train_losses, val_losses = train_model(model, train_loader, val_loader, epochs, learning_rate, patience)\n",
    "\n",
    "    # TTT:  Dynamically update model parameters in the test phase\n",
    "    test_learning_rate = 0.001\n",
    "    num_iterations = 5\n",
    "    test_predictions = test_model(model, test_loader, test_learning_rate, num_iterations)\n",
    "\n",
    "    test_predictions = test_predictions.reshape(-1, 1)\n",
    "    target_test = target_test.reshape(-1, 1)\n",
    "\n",
    "    # Inverse standardization\n",
    "    target_test_original = target_scaler.inverse_transform(target_test)\n",
    "    test_predictions_original = target_scaler.inverse_transform(test_predictions)\n",
    "\n",
    "    # Calculation index\n",
    "    test_rmse_original_scale = np.sqrt(mean_squared_error(target_test_original, test_predictions_original))\n",
    "    test_mae_original_scale = mean_absolute_error(target_test_original, test_predictions_original)\n",
    "    test_r2_original_scale = r2_score(target_test_original, test_predictions_original)\n",
    "    \n",
    "    \n",
    "    # Save the trained model\n",
    "    save_dir = os.path.abspath('/Recon_Model-main/model') \n",
    "   \n",
    "    model_filename = os.path.join(save_dir, f'model_{targ}.pkl')\n",
    "    with open(model_filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Model for {targ} saved to {model_filename}\")\n",
    "\n",
    "    # Save the thermal encoder\n",
    "    onehot_filename = os.path.join(save_dir, f'onehot_encoder_{targ}.pkl')\n",
    "    with open(onehot_filename, 'wb') as f:  \n",
    "        pickle.dump(onehot_encoder, f)\n",
    "    print(f\"OneHotEncoder saved to {onehot_filename}\")\n",
    "\n",
    "    # Save feature scaler\n",
    "    feature_scaler_filename = os.path.join(save_dir, f'feature_scaler_{targ}.pkl')\n",
    "    with open(feature_scaler_filename, 'wb') as f:\n",
    "        pickle.dump(feature_scaler, f)\n",
    "    print(f\"Feature scaler saved to {feature_scaler_filename}\")\n",
    "\n",
    "    # Save target variable scaler\n",
    "    target_scaler_filename = os.path.join(save_dir, f'target_scaler_{targ}.pkl')\n",
    "    with open(target_scaler_filename, 'wb') as f:\n",
    "        pickle.dump(target_scaler, f)  \n",
    "    print(f\"Target scaler for {targ} saved to {target_scaler_filename}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
