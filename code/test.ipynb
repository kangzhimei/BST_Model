{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from joblib import dump, load\n",
    "import os\n",
    "import pickle\n",
    "from joblib import load\n",
    "import torch.nn as nn\n",
    "from sklearn.base import BaseEstimator\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from matplotlib.lines import Line2D\n",
    "import math \n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import sys\n",
    "import warnings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.hidden_dim)\n",
    "        attention_weights = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attended_values = torch.matmul(attention_weights, value)\n",
    "\n",
    "        return attended_values\n",
    "\n",
    "class WellLoggingDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, num_layers, output_dim):\n",
    "        super(LSTMRegressor, self).__init__()\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dim1, 1, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim1 * 2, hidden_dim2, 1, batch_first=True, bidirectional=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim2 * 2, hidden_dim3, 1, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim3 * 2, output_dim)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = src.unsqueeze(1)\n",
    "\n",
    "        lstm_output1, _ = self.lstm1(src)\n",
    "        lstm_output2, _ = self.lstm2(lstm_output1)\n",
    "        lstm_output3, _ = self.lstm3(lstm_output2)\n",
    "\n",
    "        output = self.fc(lstm_output3[:, -1, :])\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在代码开始处添加\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "traindata = pd.read_csv('/Recon_Model-main/data/test.csv')\n",
    "unique_layers = traindata['Layer'].unique()\n",
    "targ_list = ['DTC']  \n",
    "layer_feature = traindata['Layer']\n",
    "\n",
    "onehot_encoder = OneHotEncoder(categories=[unique_layers], handle_unknown='ignore')\n",
    "\n",
    "for targ in targ_list:\n",
    "    print(f\"Predicting for target variable: {targ}\")\n",
    "    all_features = ['DEPTH', 'GR', 'RD', 'RS', 'DEN', 'DTC']\n",
    "    feature_names = [f for f in all_features if f != targ]\n",
    "    target = traindata[targ]\n",
    "    features = traindata[feature_names].values\n",
    "    features = np.nan_to_num(features)\n",
    "    target = np.nan_to_num(target)\n",
    "\n",
    "    # Load saved model\n",
    "    predictor = load(f'/BST_Model/model/model_{targ}.pkl')\n",
    "    scalerX = load(f'/BST_Model/model/feature_scaler_{targ}.pkl')\n",
    "    scalerY = load(f'/BST_Model/model/target_scaler_{targ}.pkl')\n",
    "    onehot_encoder = load(f'/BST_Model/model/onehot_encoder_{targ}.pkl')\n",
    "\n",
    "    # Use the loaded onehotencoder to encode the layer column independently\n",
    "    layer_onehot = onehot_encoder.transform(layer_feature.values.reshape(-1, 1)).toarray() \n",
    "    \n",
    "    # Combine the layer feature with other features\n",
    "    features_encoded = np.concatenate((features, layer_onehot), axis=1)\n",
    "    \n",
    "    # Standardize features\n",
    "    features_scaled = scalerX.transform(features_encoded)\n",
    "\n",
    "    # Prediction using loaded models\n",
    "    predictor.eval()\n",
    "    with torch.no_grad():\n",
    "        features_scaled_tensor = torch.tensor(features_scaled, dtype=torch.float32).to(device)\n",
    "        target_pred_scaled = predictor(features_scaled_tensor)\n",
    "        target_pred_scaled = target_pred_scaled.cpu().numpy()\n",
    "    \n",
    "    # Reverse scale predictor\n",
    "    target_pred = scalerY.inverse_transform(target_pred_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "    test_rmse_original_scale = np.sqrt(mean_squared_error(target, target_pred))\n",
    "    test_mae_original_scale = mean_absolute_error(target, target_pred)\n",
    "    test_r2_original_scale = r2_score(target, target_pred)\n",
    "\n",
    "    print(f\"Final Test RMSE (Original Scale): {test_rmse_original_scale}\")\n",
    "    print(f\"Final Test MAE (Original Scale): {test_mae_original_scale}\")\n",
    "    print(f\"Final Test r2 (Original Scale): {test_r2_original_scale}\")\n",
    "   \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
